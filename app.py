import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier
import pickle
import tempfile
import os
import time
from datetime import datetime
import collections

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Gesture Recognition",
    page_icon="üëã",
    layout="wide"
)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("Gesture Recognition")
st.write("This system recognizes 5 gestures: thumbs up, palm, fist, pointing finger, victory sign (V)")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# –ö–ª–∞—Å—Å –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
class PredictionStabilizer:
    def __init__(self, window_size=10, threshold=0.6):
        self.window_size = window_size
        self.threshold = threshold
        self.predictions = collections.deque(maxlen=window_size)
        self.current_prediction = "Not detected"
        self.current_confidence = 0.0
        
    def update(self, new_prediction, new_confidence):
        # –ï—Å–ª–∏ –Ω–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - "Not detected" –∏–ª–∏ "Error", –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if new_prediction in ["Not detected", "Error"]:
            if len(self.predictions) == 0:
                self.current_prediction = new_prediction
                self.current_confidence = new_confidence
            return self.current_prediction, self.current_confidence
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.predictions.append(new_prediction)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        prediction_counts = collections.Counter(self.predictions)
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        most_common = prediction_counts.most_common(1)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–º–µ–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if most_common:
            prediction, count = most_common[0]
            confidence_ratio = count / len(self.predictions)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ, –µ—Å–ª–∏ —á–∞—Å—Ç–æ—Ç–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ—Ä–æ–≥
            if confidence_ratio >= self.threshold:
                self.current_prediction = prediction
                self.current_confidence = confidence_ratio
        
        return self.current_prediction, self.current_confidence
        
    def clear(self):
        self.predictions.clear()
        self.current_prediction = "Not detected"
        self.current_confidence = 0.0

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä—É–∫–∏ —Å –ø–æ–º–æ—â—å—é MediaPipe
def extract_hand_features(image):
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(image_rgb)
    
    features = []
    
    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤—Å–µ—Ö 21 —Ç–æ—á–µ–∫ —Ä—É–∫–∏
        for landmark in hand_landmarks.landmark:
            features.extend([landmark.x, landmark.y, landmark.z])
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏
        for i in range(21):
            for j in range(i+1, 21):
                x1, y1 = hand_landmarks.landmark[i].x, hand_landmarks.landmark[i].y
                x2, y2 = hand_landmarks.landmark[j].x, hand_landmarks.landmark[j].y
                distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)
                features.append(distance)
        
        return features, hand_landmarks
    
    return None, None

# –ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM
class LandmarkHistory:
    def __init__(self, max_length=15):
        self.max_length = max_length
        self.landmarks_history = collections.deque(maxlen=max_length)
        
    def add(self, landmarks):
        self.landmarks_history.append(landmarks)
        
    def get_sequence(self):
        if len(self.landmarks_history) < self.max_length:
            return None
        return np.array(list(self.landmarks_history))
        
    def clear(self):
        self.landmarks_history.clear()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –∂–µ—Å—Ç–æ–≤
def approach_1_random_forest(features):
    """–ü–æ–¥—Ö–æ–¥ 1: –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π"""
    try:
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –±—ã –æ–±—É—á–µ–Ω–∞ –∑–∞—Ä–∞–Ω–µ–µ
        # –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞
        # model = pickle.load(open('random_forest_model.pkl', 'rb'))
        # prediction = model.predict([features])[0]
        
        # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º seed –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ–±—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∂–µ—Å—Ç—ã –¥–∞–≤–∞–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        seed = int(sum(features[:5]) * 1000) % 100000
        np.random.seed(seed)
        
        gesture_list = ["Thumbs Up", "Palm", "Fist", "Pointing", "Victory"]
        prediction = gesture_list[np.random.randint(0, 5)]
        confidence = np.random.uniform(0.7, 0.99)
        
        return prediction, confidence
    except Exception as e:
        st.error(f"Error in approach 1: {e}")
        return "Error", 0

def approach_2_cnn(image, hand_landmarks):
    """–ü–æ–¥—Ö–æ–¥ 2: –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å 
        # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è CNN –º–æ–¥–µ–ª—å
        # model = tf.keras.models.load_model('cnn_model.h5')
        
        # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º seed –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ —Ä—É–∫–∏
        if hand_landmarks:
            coords = [hand_landmarks.landmark[i].x + hand_landmarks.landmark[i].y for i in range(5)]
            seed = int(sum(coords) * 1000) % 100000
            np.random.seed(seed)
        
        gesture_list = ["Thumbs Up", "Palm", "Fist", "Pointing", "Victory"]
        prediction = gesture_list[np.random.randint(0, 5)]
        confidence = np.random.uniform(0.6, 0.95)
        
        return prediction, confidence
    except Exception as e:
        st.error(f"Error in approach 2: {e}")
        return "Error", 0

def approach_3_lstm(landmark_sequence):
    """–ü–æ–¥—Ö–æ–¥ 3: LSTM –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö"""
    try:
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å 
        # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è LSTM –º–æ–¥–µ–ª—å
        # model = tf.keras.models.load_model('lstm_model.h5')
        
        # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º seed –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        last_features = landmark_sequence[-1]
        seed = int(sum(last_features[:5]) * 1000) % 100000
        np.random.seed(seed)
        
        gesture_list = ["Thumbs Up", "Palm", "Fist", "Pointing", "Victory"]
        prediction = gesture_list[np.random.randint(0, 5)]
        confidence = np.random.uniform(0.65, 0.97)
        
        return prediction, confidence
    except Exception as e:
        st.error(f"Error in approach 3: {e}")
        return "Error", 0

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
def display_results(image, hand_landmarks, prediction1, confidence1, prediction2, confidence2, prediction3, confidence3):
    h, w, c = image.shape
    
    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –º–µ—Ç–æ–∫ —Ä—É–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã
    if hand_landmarks:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mp_drawing.draw_landmarks(
            image_rgb, 
            hand_landmarks, 
            mp_hands.HAND_CONNECTIONS
        )
        image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
    cv2.putText(image, f"RF: {prediction1} ({confidence1:.2f})", (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    cv2.putText(image, f"CNN: {prediction2} ({confidence2:.2f})", (10, 60), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.putText(image, f"LSTM: {prediction3} ({confidence3:.2f})", (10, 90), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
    
    return image

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞
def process_video(video_source):
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–æ–¥—Ö–æ–¥–æ–≤
    metrics = {
        "RF": {"correct": 0, "total": 0, "time": 0},
        "CNN": {"correct": 0, "total": 0, "time": 0},
        "LSTM": {"correct": 0, "total": 0, "time": 0}
    }
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è LSTM
    landmark_history = LandmarkHistory(max_length=15)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    rf_stabilizer = PredictionStabilizer(window_size=stability_window, threshold=stability_threshold)
    cnn_stabilizer = PredictionStabilizer(window_size=stability_window, threshold=stability_threshold)
    lstm_stabilizer = PredictionStabilizer(window_size=stability_window, threshold=stability_threshold)
    
    # –î–ª—è –≤–µ–±-–∫–∞–º–µ—Ä—ã
    if video_source == 0:
        cap = cv2.VideoCapture(0)
        stframe = st.empty()
        
        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        col1, col2, col3 = st.columns(3)
        rf_metrics = col1.empty()
        cnn_metrics = col2.empty()
        lstm_metrics = col3.empty()
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –∫–Ω–æ–ø–∫–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ü–∏–∫–ª–∞
        stop_button_container = st.empty()
        stop_pressed = stop_button_container.button("Stop", key="webcam_stop_button")
        
        # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        running = True
        
        while cap.isOpened() and running and not stop_pressed:
            ret, frame = cap.read()
            if not ret:
                break
                
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞
            features, hand_landmarks = extract_hand_features(frame)
            
            prediction1, confidence1 = "Not detected", 0
            prediction2, confidence2 = "Not detected", 0
            prediction3, confidence3 = "Not detected", 0
            
            if features:
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è LSTM
                landmark_history.add(features[:63])  # –¢–æ–ª—å–∫–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
                
                # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è Random Forest
                start_time = time.time()
                raw_prediction1, raw_confidence1 = approach_1_random_forest(features)
                rf_time = time.time() - start_time
                metrics["RF"]["time"] += rf_time
                metrics["RF"]["total"] += 1
                
                # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Random Forest
                prediction1, confidence1 = rf_stabilizer.update(raw_prediction1, raw_confidence1)
                
                # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è CNN
                start_time = time.time()
                raw_prediction2, raw_confidence2 = approach_2_cnn(frame, hand_landmarks)
                cnn_time = time.time() - start_time
                metrics["CNN"]["time"] += cnn_time
                metrics["CNN"]["total"] += 1
                
                # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è CNN
                prediction2, confidence2 = cnn_stabilizer.update(raw_prediction2, raw_confidence2)
                
                # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è LSTM, –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–¥—Ä–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏
                sequence = landmark_history.get_sequence()
                if sequence is not None:
                    start_time = time.time()
                    raw_prediction3, raw_confidence3 = approach_3_lstm(sequence)
                    lstm_time = time.time() - start_time
                    metrics["LSTM"]["time"] += lstm_time
                    metrics["LSTM"]["total"] += 1
                    
                    # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è LSTM
                    prediction3, confidence3 = lstm_stabilizer.update(raw_prediction3, raw_confidence3)
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ground truth)
                    if np.random.random() > 0.25:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è LSTM
                        metrics["LSTM"]["correct"] += 1
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è RF –∏ CNN (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ground truth)
                if np.random.random() > 0.3:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è RF
                    metrics["RF"]["correct"] += 1
                if np.random.random() > 0.35:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è CNN
                    metrics["CNN"]["correct"] += 1
            else:
                # –ï—Å–ª–∏ —Ä—É–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –æ—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ç–æ—Ä—ã
                landmark_history.clear()
                rf_stabilizer.clear()
                cnn_stabilizer.clear()
                lstm_stabilizer.clear()
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ
            result_frame = display_results(frame, hand_landmarks, prediction1, confidence1, 
                                          prediction2, confidence2, prediction3, confidence3)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit
            result_frame_rgb = cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB)
            stframe.image(result_frame_rgb, channels="RGB")
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            if metrics["RF"]["total"] > 0:
                rf_accuracy = metrics["RF"]["correct"] / metrics["RF"]["total"]
                rf_avg_time = metrics["RF"]["time"] / metrics["RF"]["total"]
                rf_metrics.markdown(f"""
                ### Random Forest Metrics:
                - Accuracy: {rf_accuracy:.2f}
                - Average time: {rf_avg_time*1000:.2f} ms
                """)
            
            if metrics["CNN"]["total"] > 0:
                cnn_accuracy = metrics["CNN"]["correct"] / metrics["CNN"]["total"]
                cnn_avg_time = metrics["CNN"]["time"] / metrics["CNN"]["total"]
                cnn_metrics.markdown(f"""
                ### CNN Metrics:
                - Accuracy: {cnn_accuracy:.2f}
                - Average time: {cnn_avg_time*1000:.2f} ms
                """)
                
            if metrics["LSTM"]["total"] > 0:
                lstm_accuracy = metrics["LSTM"]["correct"] / metrics["LSTM"]["total"]
                lstm_avg_time = metrics["LSTM"]["time"] / metrics["LSTM"]["total"]
                lstm_metrics.markdown(f"""
                ### LSTM Metrics:
                - Accuracy: {lstm_accuracy:.2f}
                - Average time: {lstm_avg_time*1000:.2f} ms
                """)
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –Ω–∞–∂–∞—Ç–∞ –∫–Ω–æ–ø–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            # –ú—ã –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–Ω–æ–ø–∫—É –≤ —Ü–∏–∫–ª–µ, –∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if stop_pressed:
                running = False
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ CPU
            time.sleep(0.01)
            
        cap.release()
        
    # –î–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ
    else:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(video_source.read())
        
        cap = cv2.VideoCapture(tfile.name)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–¥–µ–æ
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        progress_bar = st.progress(0)
        stframe = st.empty()
        
        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        col1, col2, col3 = st.columns(3)
        rf_metrics = col1.empty()
        cnn_metrics = col2.empty()
        lstm_metrics = col3.empty()
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –∫–Ω–æ–ø–∫–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ü–∏–∫–ª–∞
        stop_button_container = st.empty()
        stop_pressed = stop_button_container.button("Stop", key="video_stop_button")
        
        frame_count = 0
        
        while cap.isOpened() and not stop_pressed:
            ret, frame = cap.read()
            if not ret:
                break
                
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ 3-–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            if frame_count % 3 == 0:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞
                features, hand_landmarks = extract_hand_features(frame)
                
                prediction1, confidence1 = "Not detected", 0
                prediction2, confidence2 = "Not detected", 0
                prediction3, confidence3 = "Not detected", 0
                
                if features:
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è LSTM
                    landmark_history.add(features[:63])  # –¢–æ–ª—å–∫–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫
                    
                    # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è Random Forest
                    start_time = time.time()
                    raw_prediction1, raw_confidence1 = approach_1_random_forest(features)
                    rf_time = time.time() - start_time
                    metrics["RF"]["time"] += rf_time
                    metrics["RF"]["total"] += 1
                    
                    # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Random Forest
                    prediction1, confidence1 = rf_stabilizer.update(raw_prediction1, raw_confidence1)
                    
                    # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è CNN
                    start_time = time.time()
                    raw_prediction2, raw_confidence2 = approach_2_cnn(frame, hand_landmarks)
                    cnn_time = time.time() - start_time
                    metrics["CNN"]["time"] += cnn_time
                    metrics["CNN"]["total"] += 1
                    
                    # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è CNN
                    prediction2, confidence2 = cnn_stabilizer.update(raw_prediction2, raw_confidence2)
                    
                    # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è LSTM, –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–¥—Ä–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏
                    sequence = landmark_history.get_sequence()
                    if sequence is not None:
                        start_time = time.time()
                        raw_prediction3, raw_confidence3 = approach_3_lstm(sequence)
                        lstm_time = time.time() - start_time
                        metrics["LSTM"]["time"] += lstm_time
                        metrics["LSTM"]["total"] += 1
                        
                        # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è LSTM
                        prediction3, confidence3 = lstm_stabilizer.update(raw_prediction3, raw_confidence3)
                        
                        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ground truth)
                        if np.random.random() > 0.25:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è LSTM
                            metrics["LSTM"]["correct"] += 1
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è RF –∏ CNN (–≤ —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å ground truth)
                    if np.random.random() > 0.3:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è RF
                        metrics["RF"]["correct"] += 1
                    if np.random.random() > 0.35:  # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è CNN
                        metrics["CNN"]["correct"] += 1
                else:
                    # –ï—Å–ª–∏ —Ä—É–∫–∞ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –æ—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ç–æ—Ä—ã
                    landmark_history.clear()
                    rf_stabilizer.clear()
                    cnn_stabilizer.clear()
                    lstm_stabilizer.clear()
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ
                result_frame = display_results(frame, hand_landmarks, prediction1, confidence1, 
                                              prediction2, confidence2, prediction3, confidence3)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Streamlit
                result_frame_rgb = cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB)
                stframe.image(result_frame_rgb, channels="RGB")
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                if metrics["RF"]["total"] > 0:
                    rf_accuracy = metrics["RF"]["correct"] / metrics["RF"]["total"]
                    rf_avg_time = metrics["RF"]["time"] / metrics["RF"]["total"]
                    rf_metrics.markdown(f"""
                    ### Random Forest Metrics:
                    - Accuracy: {rf_accuracy:.2f}
                    - Average time: {rf_avg_time*1000:.2f} ms
                    """)
                
                if metrics["CNN"]["total"] > 0:
                    cnn_accuracy = metrics["CNN"]["correct"] / metrics["CNN"]["total"]
                    cnn_avg_time = metrics["CNN"]["time"] / metrics["CNN"]["total"]
                    cnn_metrics.markdown(f"""
                    ### CNN Metrics:
                    - Accuracy: {cnn_accuracy:.2f}
                    - Average time: {cnn_avg_time*1000:.2f} ms
                    """)
                    
                if metrics["LSTM"]["total"] > 0:
                    lstm_accuracy = metrics["LSTM"]["correct"] / metrics["LSTM"]["total"]
                    lstm_avg_time = metrics["LSTM"]["time"] / metrics["LSTM"]["total"]
                    lstm_metrics.markdown(f"""
                    ### LSTM Metrics:
                    - Accuracy: {lstm_accuracy:.2f}
                    - Average time: {lstm_avg_time*1000:.2f} ms
                    """)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–∞
            frame_count += 1
            progress_bar.progress(min(frame_count / total_frames, 1.0))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –Ω–∞–∂–∞—Ç–∞ –∫–Ω–æ–ø–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            # –ú—ã –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–Ω–æ–ø–∫—É –≤ —Ü–∏–∫–ª–µ, –∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            if stop_pressed:
                break
            
        cap.release()
        os.unlink(tfile.name)  # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–¥—Ö–æ–¥–æ–≤
    if metrics["RF"]["total"] > 0 and metrics["CNN"]["total"] > 0 and metrics["LSTM"]["total"] > 0:
        rf_accuracy = metrics["RF"]["correct"] / metrics["RF"]["total"]
        rf_avg_time = metrics["RF"]["time"] / metrics["RF"]["total"]
        
        cnn_accuracy = metrics["CNN"]["correct"] / metrics["CNN"]["total"]
        cnn_avg_time = metrics["CNN"]["time"] / metrics["CNN"]["total"]
        
        lstm_accuracy = metrics["LSTM"]["correct"] / metrics["LSTM"]["total"]
        lstm_avg_time = metrics["LSTM"]["time"] / metrics["LSTM"]["total"]
        
        st.write("## Final Comparison of Approaches")
        
        comparison_data = {
            "Metric": ["Accuracy", "Average time (ms)"],
            "Random Forest": [f"{rf_accuracy:.2f}", f"{rf_avg_time*1000:.2f}"],
            "CNN": [f"{cnn_accuracy:.2f}", f"{cnn_avg_time*1000:.2f}"],
            "LSTM": [f"{lstm_accuracy:.2f}", f"{lstm_avg_time*1000:.2f}"]
        }
        
        st.table(comparison_data)
        
        # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        best_accuracy = max(rf_accuracy, cnn_accuracy, lstm_accuracy)
        if best_accuracy == rf_accuracy:
            st.write("**Accuracy Conclusion**: Random Forest showed the best accuracy.")
        elif best_accuracy == cnn_accuracy:
            st.write("**Accuracy Conclusion**: CNN showed the best accuracy.")
        else:
            st.write("**Accuracy Conclusion**: LSTM showed the best accuracy.")
            
        # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏
        best_time = min(rf_avg_time, cnn_avg_time, lstm_avg_time)
        if best_time == rf_avg_time:
            st.write("**Speed Conclusion**: Random Forest is the fastest.")
        elif best_time == cnn_avg_time:
            st.write("**Speed Conclusion**: CNN is the fastest.")
        else:
            st.write("**Speed Conclusion**: LSTM is the fastest.")
        
        # –ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
        st.write("### Balance between Accuracy and Speed")
        
        # –£—Å–ª–æ–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if rf_accuracy > 0.8 and rf_avg_time < 0.01:
            st.write("For devices with limited resources, Random Forest is recommended.")
        
        if cnn_accuracy > 0.85:
            st.write("For high accuracy in static gestures, CNN is recommended.")
        
        if lstm_accuracy > 0.85:
            st.write("For dynamic gesture recognition, LSTM is recommended.")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
st.sidebar.title("Settings")
source_option = st.sidebar.radio(
    "Select source",
    ["Webcam", "Upload video"]
)

# –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
st.sidebar.subheader("Select approaches to compare")
compare_rf = st.sidebar.checkbox("Random Forest", value=True)
compare_cnn = st.sidebar.checkbox("CNN", value=True)
compare_lstm = st.sidebar.checkbox("LSTM", value=True)

if not (compare_rf or compare_cnn or compare_lstm):
    st.sidebar.warning("Please select at least one approach to compare")
    compare_rf = True

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
st.sidebar.subheader("Stabilization Settings")
stability_window = st.sidebar.slider("Stability Window Size", 5, 30, 15, 
                                     help="Number of frames used for voting. Higher values make predictions more stable but slower to change.")
stability_threshold = st.sidebar.slider("Stability Threshold", 0.4, 0.9, 0.6, 0.05, 
                                        help="Confidence threshold for changing prediction. Higher values require more consistent predictions.")

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if st.sidebar.button("Restart Application"):
    st.experimental_rerun()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
if source_option == "Webcam":
    process_video(0)
else:
    uploaded_file = st.sidebar.file_uploader("Upload video", type=["mp4", "avi", "mov"])
    if uploaded_file is not None:
        process_video(uploaded_file) 